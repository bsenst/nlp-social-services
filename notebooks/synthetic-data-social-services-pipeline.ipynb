{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-12T15:18:34.477065Z","iopub.status.busy":"2024-08-12T15:18:34.476661Z","iopub.status.idle":"2024-08-12T15:18:34.763834Z","shell.execute_reply":"2024-08-12T15:18:34.762808Z","shell.execute_reply.started":"2024-08-12T15:18:34.477037Z"},"trusted":true},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import spacy\n","import nltk\n","import re\n","from collections import Counter\n","from lexical_diversity import lex_div as ld\n","\n","# Download NLTK stopwords dataset\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Synthetic Data"]},{"cell_type":"markdown","metadata":{},"source":["## Open Training Dataset Social Services"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-12T15:12:49.762446Z","iopub.status.busy":"2024-08-12T15:12:49.761827Z","iopub.status.idle":"2024-08-12T15:12:49.788452Z","shell.execute_reply":"2024-08-12T15:12:49.787197Z","shell.execute_reply.started":"2024-08-12T15:12:49.762385Z"},"trusted":true},"outputs":[],"source":["# Path to your .txt file\n","file_path = '../data/synthetic-data-social-services.txt'\n","\n","# Open the file and read its contents\n","with open(file_path, 'r') as file:\n","    raw_data = file.read()\n","\n","# Split the raw data into entries based on '\\n\\n'\n","entries = raw_data.strip().split('\\n\\n')\n","\n","# Initialize lists to hold titles and contents\n","titles = []\n","contents = []\n","\n","# Process each entry\n","for entry in entries:\n","    # Split each entry into title and content based on '\\n'\n","    lines = entry.split('\\n', 1)  # Split only on the first '\\n'\n","    if len(lines) == 2:\n","        title, content = lines\n","        titles.append(title)\n","        contents.append(content)\n","    else:\n","        # Handle cases where the entry does not have a proper title/content split\n","        titles.append(lines[0])\n","        contents.append('')\n","\n","# Create a DataFrame from the lists\n","df_train = pd.DataFrame({\n","    'title': titles,\n","    'content': contents\n","})\n","\n","print(df_train.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Add Manually Labeled Service Categories"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:12:49.789944Z","iopub.status.busy":"2024-08-12T15:12:49.789613Z","iopub.status.idle":"2024-08-12T15:12:49.807170Z","shell.execute_reply":"2024-08-12T15:12:49.805941Z","shell.execute_reply.started":"2024-08-12T15:12:49.789911Z"},"trusted":true},"outputs":[],"source":["# Path to your .txt file\n","file_path = '../data/synthetic-data-social-services-categories.txt'\n","\n","# Open the file and read its contents\n","with open(file_path, 'r') as file:\n","    raw_data = file.read()\n","    \n","# Repeat each term 10 times\n","df_train['category'] = [term for term in raw_data.split('\\n') for _ in range(10)]"]},{"cell_type":"markdown","metadata":{},"source":["## Add General Website Content"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:12:49.810465Z","iopub.status.busy":"2024-08-12T15:12:49.809980Z","iopub.status.idle":"2024-08-12T15:12:49.821865Z","shell.execute_reply":"2024-08-12T15:12:49.820728Z","shell.execute_reply.started":"2024-08-12T15:12:49.810424Z"},"trusted":true},"outputs":[],"source":["# Path to your .txt file\n","file_path = '../data/synthetic-data-press-releases.txt'\n","\n","# Open the file and read its contents\n","with open(file_path, 'r') as file:\n","    raw_data = file.read()\n","\n","# Split the raw data into entries based on '\\n\\n'\n","entries = raw_data.strip().split('\\n\\n')\n","\n","# Initialize lists to hold titles and contents\n","titles = [el.split('\\n')[0] for el in entries[::2]]\n","contents = entries[1::2]\n","\n","# Create a DataFrame from the lists\n","df_test = pd.DataFrame({\n","    'title': titles,\n","    'content': contents\n","})\n","\n","print(df_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:12:49.823626Z","iopub.status.busy":"2024-08-12T15:12:49.823267Z","iopub.status.idle":"2024-08-12T15:12:49.837270Z","shell.execute_reply":"2024-08-12T15:12:49.836039Z","shell.execute_reply.started":"2024-08-12T15:12:49.823592Z"},"trusted":true},"outputs":[],"source":["# Path to your .txt file\n","file_path = '../data/synthetic-data-non-service-texts.txt'\n","\n","# Open the file and read its contents\n","with open(file_path, 'r') as file:\n","    raw_data = file.read()\n","    \n","# Split the raw data into entries based on '\\n\\n'\n","entries = [el.split('\\n') for el in raw_data.strip().split('\\n\\n')]\n","\n","# Initialize lists to hold titles and contents\n","titles = [el[0] for el in entries]\n","contents = [el[1] for el in entries]\n","\n","# Create a DataFrame from the lists\n","df_noise = pd.DataFrame({\n","    'title': titles,\n","    'content': contents\n","})\n","\n","print(df_noise.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Raw Web Data Social Services"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:13:49.803198Z","iopub.status.busy":"2024-08-12T15:13:49.802423Z","iopub.status.idle":"2024-08-12T15:13:49.835965Z","shell.execute_reply":"2024-08-12T15:13:49.834737Z","shell.execute_reply.started":"2024-08-12T15:13:49.803155Z"},"trusted":true},"outputs":[],"source":["# Concatenate DataFrames row-wise\n","combined_df = pd.concat([df_train[['title', 'content']], df_test, df_noise, df_noise, df_noise], ignore_index=True)\n","\n","# Step 4: Shuffle the DataFrame\n","shuffled_df = combined_df.sample(frac=1, random_state=1).reset_index(drop=True)\n","\n","shuffled_df"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["## Raw Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:16:04.076803Z","iopub.status.busy":"2024-08-12T15:16:04.075905Z","iopub.status.idle":"2024-08-12T15:16:04.081807Z","shell.execute_reply":"2024-08-12T15:16:04.080780Z","shell.execute_reply.started":"2024-08-12T15:16:04.076767Z"},"trusted":true},"outputs":[],"source":["print(f'Imagine the web scraping dataset contains {shuffled_df.shape[0]} web documents.')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:16:23.726976Z","iopub.status.busy":"2024-08-12T15:16:23.726586Z","iopub.status.idle":"2024-08-12T15:16:24.312183Z","shell.execute_reply":"2024-08-12T15:16:24.311131Z","shell.execute_reply.started":"2024-08-12T15:16:23.726947Z"},"trusted":true},"outputs":[],"source":["# Function to tokenize text into words\n","def simple_word_tokenize(text):\n","    words = re.findall(r'\\b\\w+\\b', text.lower())\n","    return words\n","\n","# Combine content and titles\n","combined_texts = [title + \" \" + content for title, content in zip(shuffled_df[\"title\"], shuffled_df[\"content\"])]\n","\n","# Tokenize the combined texts and calculate term frequencies\n","all_words = []\n","for text in combined_texts:\n","    all_words.extend(simple_word_tokenize(text))\n","\n","# Calculate term frequencies\n","term_frequencies = Counter(all_words)\n","\n","# Sort and select the top 15 most common terms\n","top_n = 15\n","top_terms = term_frequencies.most_common(top_n)\n","\n","# Separate the terms and their frequencies for plotting\n","terms, frequencies = zip(*top_terms)\n","\n","# # Plot the term frequencies as a bar plot\n","# plt.figure(figsize=(6, 4))\n","# plt.barh(terms, frequencies, color='skyblue')\n","# plt.xlabel('Frequency')\n","# plt.ylabel('Term')\n","# plt.title(f'Top {top_n} Term Frequencies')\n","# plt.gca().invert_yaxis()  # Invert y-axis to have the highest frequency on top\n","\n","# # Display the plot\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Remove Duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:18:19.008305Z","iopub.status.busy":"2024-08-12T15:18:19.007920Z","iopub.status.idle":"2024-08-12T15:18:19.015898Z","shell.execute_reply":"2024-08-12T15:18:19.014730Z","shell.execute_reply.started":"2024-08-12T15:18:19.008278Z"},"trusted":true},"outputs":[],"source":["shuffled_df.drop_duplicates(inplace=True)\n","\n","print(f'After removing duplicates, {shuffled_df.shape[0]} documents are left')"]},{"cell_type":"markdown","metadata":{},"source":["## Lexical Diversity"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:18:55.726230Z","iopub.status.busy":"2024-08-12T15:18:55.725792Z","iopub.status.idle":"2024-08-12T15:18:55.934299Z","shell.execute_reply":"2024-08-12T15:18:55.933346Z","shell.execute_reply.started":"2024-08-12T15:18:55.726193Z"},"trusted":true},"outputs":[],"source":["lex_div = shuffled_df.content.apply(ld.tokenize).apply(ld.root_ttr)\n","\n","# # Plotting the MSTTR values as a histogram\n","# plt.figure(figsize=(4, 2))\n","# plt.hist(lex_div, color='skyblue')\n","# plt.xlabel('Root Type Token Ratio')\n","# plt.ylabel('Website Count')\n","# plt.title('Root TTR of Website Texts')\n","\n","# # Display the plot\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:19:03.117758Z","iopub.status.busy":"2024-08-12T15:19:03.117340Z","iopub.status.idle":"2024-08-12T15:19:03.129457Z","shell.execute_reply":"2024-08-12T15:19:03.128280Z","shell.execute_reply.started":"2024-08-12T15:19:03.117724Z"},"trusted":true},"outputs":[],"source":["# Filter Web documents with high Root TTR\n","cleaned_df = shuffled_df[lex_div>4]\n","\n","cleaned_df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:19:06.878712Z","iopub.status.busy":"2024-08-12T15:19:06.878311Z","iopub.status.idle":"2024-08-12T15:19:06.891101Z","shell.execute_reply":"2024-08-12T15:19:06.890036Z","shell.execute_reply.started":"2024-08-12T15:19:06.878683Z"},"trusted":true},"outputs":[],"source":["# Removed web documents, having low value content\n","shuffled_df[lex_div<4].head(5)"]},{"cell_type":"markdown","metadata":{},"source":["## Remove Stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:21:27.685420Z","iopub.status.busy":"2024-08-12T15:21:27.685009Z","iopub.status.idle":"2024-08-12T15:21:28.006268Z","shell.execute_reply":"2024-08-12T15:21:28.005213Z","shell.execute_reply.started":"2024-08-12T15:21:27.685375Z"},"trusted":true},"outputs":[],"source":["# List of German stopwords\n","german_stopwords = stopwords.words('german')\n","\n","# Combine content and titles\n","combined_texts = [title + \" \" + content for title, content in zip(cleaned_df[\"title\"], cleaned_df[\"content\"])]\n","\n","# Tokenize the combined texts and calculate term frequencies, excluding stopwords\n","all_words = []\n","for text in combined_texts:\n","    words = simple_word_tokenize(text)\n","    filtered_words = [word for word in words if word not in german_stopwords]\n","    all_words.extend(filtered_words)\n","\n","# Calculate term frequencies\n","term_frequencies = Counter(all_words)\n","\n","# Sort and select the top 15 most common terms\n","top_n = 15\n","top_terms = term_frequencies.most_common(top_n)\n","\n","# Separate the terms and their frequencies for plotting\n","terms, frequencies = zip(*top_terms)\n","\n","# # Plot the term frequencies as a bar plot\n","# plt.figure(figsize=(6, 4))\n","# plt.barh(terms, frequencies, color='skyblue')\n","# plt.xlabel('Frequency')\n","# plt.ylabel('Term')\n","# plt.title(f'Top {top_n} Term Frequencies (Excluding Stopwords)')\n","# plt.gca().invert_yaxis()  # Invert y-axis to have the highest frequency on top\n","\n","# # Display the plot\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Processing"]},{"cell_type":"markdown","metadata":{},"source":["## Anonymization with Entity Recognition\n","\n","The synthetic dataset has been prepared making sure no personal identifable information is contained."]},{"cell_type":"markdown","metadata":{},"source":["## Social Service Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tmp = pd.DataFrame()\n","\n","tmp['text'] = [title + \" \" + content for title, content in zip(df_train[\"title\"], df_train[\"content\"])]\n","tmp['cats'] = df_train['category']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T15:23:08.076083Z","iopub.status.busy":"2024-08-12T15:23:08.075634Z","iopub.status.idle":"2024-08-12T15:23:19.978587Z","shell.execute_reply":"2024-08-12T15:23:19.977183Z","shell.execute_reply.started":"2024-08-12T15:23:08.076052Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","# Function to transform the DataFrame row\n","def transform_row(row):\n","\n","    if row['cats'] == 'Obdachlosenhilfe':\n","        cats = {\n","            'Obdachlosenhilfe': 1.0,\n","            \"OTHER\": 0.0\n","        }\n","    else: \n","        cats = {\n","            'Obdachlosenhilfe': 0.0,\n","            \"OTHER\": 1.0\n","        }\n","    \n","    return {\n","        \"text\": row['text'],\n","        \"cats\": cats\n","    }\n","\n","# Apply the transformation to each row\n","transformed_data = tmp.apply(transform_row, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import random\n","\n","# Convert transformed_data to a list for easy manipulation\n","transformed_data_list = transformed_data.tolist()\n","\n","# Split the data\n","train_data = [transformed_data_list[i] for i in range(len(transformed_data_list)) if i % 10 in range(2, 10)]\n","test_data = [transformed_data_list[i] for i in range(len(transformed_data_list)) if i % 10 in [0, 1]]\n","\n","# Write training data to a JSONL file\n","with open('../textcat_demo/assets/docs_issues_training.jsonl', 'w') as f:\n","    for record in train_data:\n","        f.write(json.dumps(record) + '\\n')\n","\n","# Write test data to a JSONL file\n","with open('../textcat_demo/assets/docs_issues_eval.jsonl', 'w') as f:\n","    for record in test_data:\n","        f.write(json.dumps(record) + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ! cd ../textcat_demo/ && weasel run all"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ! cd ../textcat_demo/ && weasel run visualize-model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the spaCy model from the directory\n","nlp = spacy.load(\"../textcat_demo/training/model-best\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Process a text\n","doc = nlp(\"In der Übernachtungsstätte gibt es 30 Plätze für Wohnungslose Menschen.\")\n","\n","# Access predictions (e.g., text categorization)\n","print(doc.cats)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Process a text\n","doc = nlp(\"Die Pflegeeinrichtung begrüßt ihre Gäste.\")\n","\n","# Access predictions (e.g., text categorization)\n","print(doc.cats)"]},{"cell_type":"markdown","metadata":{},"source":["## Homeless Relief"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine all texts into a single string\n","text = \" \".join(df_train[df_train.category=='Obdachlosenhilfe'].content)\n","\n","# Create the word cloud, removing German stopwords\n","wordcloud = WordCloud(\n","    stopwords=german_stopwords,\n","    background_color='white',\n","    width=800,\n","    height=400\n",").generate(text)\n","\n","# # Display the word cloud using Matplotlib\n","# plt.figure(figsize=(6, 3))\n","# plt.imshow(wordcloud, interpolation='bilinear')\n","# plt.axis('off')\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Social Services"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine all texts into a single string\n","text = \" \".join(df_train[df_train.category=='Seniorenhilfe'].content)\n","\n","# Create the word cloud, removing German stopwords\n","wordcloud = WordCloud(\n","    stopwords=german_stopwords,\n","    background_color='white',\n","    width=800,\n","    height=400\n",").generate(text)\n","\n","# # Display the word cloud using Matplotlib\n","# plt.figure(figsize=(6, 3))\n","# plt.imshow(wordcloud, interpolation='bilinear')\n","# plt.axis('off')\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Applications"]},{"cell_type":"markdown","metadata":{},"source":["## Keyword Search"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from whoosh.fields import Schema, TEXT\n","from whoosh.index import create_in\n","import os\n","\n","# Define the schema\n","schema = Schema(id=TEXT(stored=True), text=TEXT(stored=True))\n","\n","# Create a directory to store the index\n","if not os.path.exists(\"indexdir\"):\n","    os.mkdir(\"indexdir\")\n","\n","# Create the index\n","index = create_in(\"indexdir\", schema)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from whoosh.writing import AsyncWriter\n","\n","# Open the index for writing\n","with index.writer() as writer:\n","    for id, text in enumerate(combined_texts):\n","        writer.add_document(id=str(id), text=text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from whoosh.qparser import QueryParser\n","from whoosh import scoring\n","\n","# Open the index for searching\n","with index.searcher() as searcher:\n","    # Create a query parser\n","    query_parser = QueryParser(\"text\", schema=schema)\n","    \n","    # Define a query\n","    query = query_parser.parse(\"Obdachlose\")\n","    \n","    # Perform the search\n","    results = searcher.search(query, limit=3)\n","    \n","    # Print results\n","    for result in results:\n","        print(f\"ID: {result['id']}, Text: {result['text']}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Creating a Map"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example services with their imaginary coordinates (x, y)\n","services = {\n","    \"Niederschwelliger Tagesaufenthalt\": (2, 5),\n","    \"Übernachtungseinrichtung\": (4, 9),\n","    \"Streetwork-Programm\": (7, 2),\n","    \"Sozialberatung\": (6, 8),\n","    \"Wärmestube\": (9, 5)\n","}\n","\n","# Plotting the map\n","plt.figure(figsize=(8, 8))\n","for service, (x, y) in services.items():\n","    plt.scatter(x, y, s=100)  # Plot the point\n","    plt.text(x + 0.1, y + 0.1, service, fontsize=10)  # Add label with some offset\n","\n","# Adding titles and labels\n","plt.title('Imaginary Map of Services')\n","plt.xlabel('X Coordinate')\n","plt.ylabel('Y Coordinate')\n","\n","# Adding grid and setting limits for better visualization\n","plt.grid(True)\n","plt.xlim(0, 10)\n","plt.ylim(0, 10)\n","\n","# Show the plot\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5525924,"sourceId":9148506,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
